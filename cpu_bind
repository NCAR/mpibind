#!/bin/bash

# tmp_logfile should be set int the mpibind calling script
if [ -z "$tmp_logfile" ]; then
    echo "ERROR: tmp_logfile is undefined, it should be set by mpibind!"
    echo "($0 is not intended to be called directly)"
    exit 1
fi

# get rank info depending on the MPI library being used
if [ $LMOD_FAMILY_MPI == "cray-mpich" ]; then
  grank=$PMI_RANK
  lrank=$PMI_LOCAL_RANK
  lsize=$PMI_LOCAL_SIZE
elif [ $LMOD_FAMILY_MPI == "openmpi" ]; then
  grank=$OMPI_COMM_WORLD_RANK
  lrank=$OMPI_COMM_WORLD_LOCAL_RANK
  lsize=$OMPI_COMM_WORLD_LOCAL_SIZE
elif [ $LMOD_FAMILY_MPI == "intel-mpi" ]; then
  grank=$PMI_RANK
  lrank=$MPI_LOCALRANKID
  lsize=$MPI_LOCALNRANKS
else
  >&2 echo "Error: mpibind requires either cray-mpich or intel-mpi for binding CPU applications"
  exit 2
fi

# set OMP environment variables if not set
if [[ -z "$OMP_NUM_THREADS" ]]; then
  export OMP_NUM_THREADS=1
fi
if [[ -z "$OMP_PLACES" ]]; then
   export OMP_PLACES=threads
fi
if [[ -z "$OMP_PROC_BIND" ]]; then
   export OMP_PROC_BIND=close
fi
nthds=$OMP_NUM_THREADS

# support two different binding scenarios
# 1. Exclusive use of the nodes - split ranks between sockets
# 2. Shared use of the nodes - bind using only cores allocated by PBS

# get a hardware view of the node
nsock=`lscpu | grep Socket | awk '{print $2}'`
cpsock=`lscpu | grep "per sock" | awk '{print $4}'`
hardware_npcores=$((nsock*cpsock)) # total number of physical cores in the node (ignoring hyperthreads)
# get the numactl view of the node
nctl_corelist=`numactl -s | grep physcpubind | egrep -o "[0-9]+"`   # list of logical cores from numactl
nctl_ncores=`echo $nctl_corelist | wc -w`                           # number of logical cores from numactl (will be even coming from PBS)
nctl_npcores=`echo $((nctl_ncores/2))`                              # number of physical cores from numactl (safely divisible by 2)
# check if the number of physical cores from numactl is consistent with the hardware view
if [ $nctl_npcores -ne $hardware_npcores ]; then
  # bind for shared node usage
  # have rank 0 print a warning message about the shared node assumption
  if [ $grank -eq 0 ]; then
    echo "Warning: Assuming shared node usage and binding to $nctl_npcores of the $hardware_npcores available CPU cores"
  fi
  ranges=()
  for i in `seq 0 $nthds $((nctl_npcores-1))`; do
    ranges+=("$i-$((i+nthds-1))")
  done
else
  # bind for exclusive node usage, bind with ranks split between sockets
  rank_per_sock=$((lsize/nsock + (lsize%nsock)))
  thrd_per_sock=$((rank_per_sock*nthds))
  s0ranges=()
  s1ranges=()
  for i in `seq 0 $nthds $((thrd_per_sock - 1))`; do
    s0ranges+=("$i-$((i+nthds-1))")
    s1ranges+=("$((i+cpsock))-$((i+cpsock+nthds-1))")
  done
  ranges=("${s0ranges[@]}" "${s1ranges[@]}")
fi

echo "rank: $grank, cores: ${ranges[lrank]}" > $tmp_logfile.rankinfo.$grank
numactl -C "${ranges[lrank]}" $*
