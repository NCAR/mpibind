#!/bin/bash

# utility function to produce sequences with the possibility of
# alternating strides. Needed when the number of ranks on a CPU
# does not evenly divide the number of cores on a CPU.
seq_alt_stride() {
  local start=0
  local count=$1
  local stride1=$2
  local stride2=$((stride1 + $3))
  local current=$start

  for ((i=0; i<count; i++)); do
    printf "%d " "$current"
    if (( i % 2 == 0 )); then
      current=$(( current + stride1 ))
    else
      current=$(( current + stride2 ))
    fi
  done
  echo
}

# tmp_logdir should be set int the mpibind calling script
if [ $enable_logs -eq 1 ]; then
  if [ -z "$tmp_logdir" ]; then
      echo "ERROR: tmp_logdir is undefined, it should be set by mpibind!"
      echo "($0 is not intended to be called directly)"
      exit 1
  fi
fi

# Set variables used in binding calculations according to the MPI family
if [ $LMOD_FAMILY_MPI == "cray-mpich" ]; then
  export MPICH_GPU_SUPPORT_ENABLED=1
  grank=$PMI_RANK
  lrank=$PMI_LOCAL_RANK
  lsize=$PMI_LOCAL_SIZE
elif [ $LMOD_FAMILY_MPI == "openmpi" ]; then
  grank=$OMPI_COMM_WORLD_RANK
  lrank=$OMPI_COMM_WORLD_LOCAL_RANK
  lsize=$OMPI_COMM_WORLD_LOCAL_SIZE
else
  >&2 echo "Error: mpibind requires cray-mpich or openmpi for binding GPU applications"
  exit 2
fi

# Set the NIC policy to GPU by default (MPICH specific, but harmless for OpenMPI)
if [[ -z "$MPICH_OFI_NIC_POLICY" ]]; then
   export MPICH_OFI_NIC_POLICY=GPU
fi

# hardware view of the node
nsock=`lscpu | grep Socket | awk '{print $2}'`
cpsock=`lscpu | grep "per sock" | awk '{print $4}'`
numanodes=`lscpu | grep "NUMA node(s)" | awk '{print $3}'`  # number of NUMA nodes
hardware_npcores=$((nsock*cpsock))                          # total number of physical cores in the node (ignoring hyperthreads)
hardware_ngpus=`ls /proc/driver/nvidia/gpus/ | wc -l`       # total number of GPUs in the node
# the numactl view of the node
nctl_corelist=(`numactl -s | grep physcpubind | egrep -o "[0-9]+"`)                                  # array of logical cores from numactl
nctl_pcorelist=(`echo "${nctl_corelist[@]}" | awk 'BEGIN{ RS=" " } $0 < max' max=$hardware_npcores`) # array of physical cores from numactl
nctl_npcores=${#nctl_pcorelist[@]}                                                                   # number of physical cores from numactl
nvsmi_ngpus=`nvidia-smi -L | wc -l`                                                                  # number of GPUs assigned from nvidia-smi

# check if the number of physical cores from numactl is consistent with the hardware
# and if the number of gpus from nvidia-smi is consistent with the hardware
# to determine if we can bind for exclusive node usage and optimal affinity
if [ $nctl_npcores -lt $hardware_npcores ] || [ $nvsmi_ngpus -lt $hardware_ngpus ]; then
  # bind for shared node usage
  # have rank 0 print a warning message about the shared node assumption
  if [ $grank -eq 0 ]; then
    echo "Binding for shared node usage on $nctl_npcores of the $hardware_npcores available CPU cores"
    echo "                             and $nvsmi_ngpus of the $hardware_ngpus available GPUs. Affinity may not be optimal."
  fi
  cores=(${nctl_pcorelist[@]})
  local_gpus=(`nvidia-smi -L | egrep -o "GPU\s+[0-9]+" | awk '{print $2}'`)
  local_gpu=${local_gpus[lrank%${#local_gpus[@]}]}
else
  # bind for exclusive node usage
  # generate a list of cores that will be used to bind ranks
  cores=()
  # about as simple as I can make it without losing generality
  corelist=`seq_alt_stride lsize $((nctl_npcores/lsize)) $((nctl_npcores%lsize != 0))`
  for i in $corelist; do
    cores+=("$i")
  done

  # List the GPUs with affinity for the core this rank will be bound to
  local_gpu_list=`nvidia-smi topo -c ${cores[lrank]} | sed s/,//g | egrep -v [a-z]`
  local_gpus=()
  for i in $local_gpu_list; do
    local_gpus+=("$i")
  done
  # select the GPU for this rank.
  local_gpu=$((local_gpus[lrank%(lsize/numanodes)]))
fi

# Restrict this rank to the selected GPU and bind to the selected core
export CUDA_VISIBLE_DEVICES=$local_gpu
export MPIBIND_GPU_UUID=`nvidia-smi -L | grep "GPU $CUDA_VISIBLE_DEVICES:" | egrep -o "GPU-[a-f,0-9]+-[a-f,0-9]+-[a-f,0-9]+-[a-f,0-9]+-[a-f,0-9]+"`
if [ $enable_logs -eq 1 ]; then
  echo "rank: $grank, cores: ${cores[lrank]}, CUDA DEVICE: $CUDA_VISIBLE_DEVICES, UUID: $MPIBIND_GPU_UUID" > $tmp_logdir/rankinfo.$grank
fi

exec numactl -C "${cores[lrank]}" $*
