#!/bin/bash

# tmp_logdir should be set in the mpibind calling script
if [ $enable_logs -eq 1 ]; then
  if [ -z "$tmp_logdir" ]; then
      echo "ERROR: tmp_logdir is undefined, it should be set by mpibind!"
      echo "($0 is not intended to be called directly)"
      exit 1
  fi
fi

# Set variables used in binding calculations according to the MPI family
if [ $LMOD_FAMILY_MPI == "cray-mpich" ]; then
  export MPICH_GPU_SUPPORT_ENABLED=1
  grank=$PMI_RANK
  lrank=$PMI_LOCAL_RANK
  lsize=$PMI_LOCAL_SIZE
elif [ $LMOD_FAMILY_MPI == "openmpi" ]; then
  grank=$OMPI_COMM_WORLD_RANK
  lrank=$OMPI_COMM_WORLD_LOCAL_RANK
  lsize=$OMPI_COMM_WORLD_LOCAL_SIZE
else
  >&2 echo "Error: mpibind requires cray-mpich or openmpi for binding GPU applications"
  exit 2
fi

# Set the NIC policy to GPU by default (MPICH specific, but harmless for OpenMPI)
if [[ -z "$MPICH_OFI_NIC_POLICY" ]]; then
   export MPICH_OFI_NIC_POLICY=GPU
fi

# hardware view of the node
nsock=`lscpu | grep Socket | awk '{print $2}'`
cpsock=`lscpu | grep "per sock" | awk '{print $4}'`
hardware_npcores=$((nsock*cpsock))                          # total number of physical cores in the node (ignoring hyperthreads)
hardware_ngpus=`ls /proc/driver/nvidia/gpus/ | wc -l`       # total number of GPUs in the node
# the numactl and nvidia-smi view of the node
nctl_corelist=(`numactl -s | grep physcpubind | egrep -o "[0-9]+"`)                                  # array of logical cores from numactl
nctl_pcorelist=(`echo "${nctl_corelist[@]}" | awk 'BEGIN{ RS=" " } $0 < max' max=$hardware_npcores`) # array of physical cores from numactl
nctl_npcores=${#nctl_pcorelist[@]}                                                                   # number of physical cores from numactl
nvsmi_ngpus=`nvidia-smi -L | wc -l`                                                                  # number of GPUs assigned from nvidia-smi

# check if the number of physical cores from numactl is consistent with the hardware
# and if the number of gpus from nvidia-smi is consistent with the hardware
# to determine if we can bind for exclusive node usage and optimal affinity
if [ $nctl_npcores -lt $hardware_npcores ] || [ $nvsmi_ngpus -lt $hardware_ngpus ]; then
  # bind for shared node usage
  # have rank 0 print a warning message about the shared node assumption
  if [ $grank -eq 0 ]; then
    echo "Binding for shared node usage on $nctl_npcores of the $hardware_npcores available CPU cores"
    echo "                             and $nvsmi_ngpus of the $hardware_ngpus available GPUs. Affinity may not be optimal."
  fi
  cores=(${nctl_pcorelist[@]})
  local_gpus=(`nvidia-smi -L | egrep -o "GPU\s+[0-9]+" | awk '{print $2}'`)
  local_gpu=${local_gpus[lrank%${#local_gpus[@]}]}
  local_core=${cores[lrank%${#cores[@]}]}
else
  # bind for exclusive node usage

  # calculate the number of ranks bound to each GPU
  # we are not assuming all GPUs have the same number of ranks bound to them
  base=$((lsize/nvsmi_ngpus));
  rem=$((lsize%nvsmi_ngpus));
  
  # the below nested loop creates a list of the GPUs local to each rank
  local_gpus=()
  rank=0;
  for (( g=0; g<nvsmi_ngpus; g++ )); do
    count=$base;
    if [ $g -lt $rem ]; then
      count=$(($count+1));
    fi

    for ((i=0; i<$count; i++)); do
      local_gpus+=($g);
      rank=$((rank+1));
    done
  done
  # set the local GPU for this rank
  local_gpu=${local_gpus[lrank]}

  # find all the ranks that will be bound to this GPU
  gpu_ranks=()
  for r in `seq 0 $(( lsize - 1 ))`; do
    if [ ${local_gpus[r]} -eq $local_gpu ]; then
      gpu_ranks+=($r)
    fi
  done

  # Find the cores with affinity for this GPU. On some systems cores have affinity 
  # for multiple GPUs. The pos_offset variable records which position the GPU 
  # matched in the nvidia-smi topo output, and is used later to calculate an offset
  # into the local_cores array to ensure unique core <--> GPU bindings
  local_cores=()
  for c in `seq 0 $(( nctl_npcores - 1 ))`; do 
    gpus=(`nvidia-smi topo -c $c | egrep "^[0-9]+" | sed "s/,//g"`)
    for i in `seq 0 $(( ${#gpus[@]} - 1 ))`; do
      if [ "${gpus[$i]}" == "$local_gpu" ]; then
        local_cores+=("$c")
        pos_offset=$i
      fi
    done
  done

  # Calculate an offset into the local_cores array so each rank is bound
  # to a different core among the cores with affinity for this GPU
  for r in `seq 0 ${#gpu_ranks[@]}`; do
    if [ ${gpu_ranks[r]} -eq $lrank ]; then
      core_index=$(( (r + base*pos_offset) % ${#local_cores[@]} ))
      break
    fi
  done

  # select the CPU core to bind this rank to
  # this core has affinity to the local_gpu
  local_core=${local_cores[core_index]}
fi

# Restrict this rank to the selected GPU and bind to the selected core
export CUDA_VISIBLE_DEVICES=$local_gpu
export MPIBIND_GPU_UUID=`nvidia-smi -L | grep "GPU $CUDA_VISIBLE_DEVICES:" | egrep -o "GPU-[a-f,0-9]+-[a-f,0-9]+-[a-f,0-9]+-[a-f,0-9]+-[a-f,0-9]+"`
if [ $enable_logs -eq 1 ]; then
  echo "rank: $grank, cores: ${cores[lrank]}, CUDA DEVICE: $CUDA_VISIBLE_DEVICES, UUID: $MPIBIND_GPU_UUID" > $tmp_logdir/rankinfo.$grank
fi

exec numactl -C "$local_core" $*